### Can humans help BERT gain "confidence"?

The advancements in artificial intelligence over the last decade have opened a multitude of
avenues for interdisciplinary research. Since the idea of artificial intelligence was inspired by the
working of neurons in the brain, it seems quite practical to combine the two fields and take the
help of cognitive data to train AI models. Not only it will help to get a deeper understanding of
the technology, but the brain as well. In this thesis, I conduct novel experiments to integrate
cognitive features from the Zurich Cognitive Corpus (ZuCo) (Hollenstein et al., 2018) with a
transformer-based encoder model called BERT. I show how EEG and eye-tracking features from
ZuCo can help to increase the performance of the NLP model. I confirm the performance increase
with the help of a robustness-checking pipeline and derive a word-EEG lexicon to use in
benchmarking an external dataset that does not have any cognitive features associated with it.
Further, I analyze the internal working mechanism of BERT and explore a potential method for
model explainability by correlating it with a popular model agnostic explainability framework
called LIME (Ribeiro et al., 2016). Finally, I discuss the possible directions to take this research
forward.

###Please open an issue to get access to the thesis.
